Regression and Uncertainty Sets: Hourly Load
========================================================

Uses system wide hourlyload for ISO-NE from June 2003 through June 2013.  Restricts analysis to summer months, week days.  Withhold June 2013 for later validation.  Pre-2010 is used for training.  Post 2010 is used for testing. 
```{r, echo=FALSE, message=FALSE}
library(gdata)
library(ggplot2)
library(reshape)

setwd("/Users/VGupta/Documents/Research/UnitCommittment/ISO-NE/")
load_hist.orig = read.csv('load_hist_orig.csv')
load_hist.orig$dateTime <- as.POSIXct(load_hist.orig$dateTime)
load_hist.orig <- load_hist.orig[order(load_hist.orig$Date, load_hist.orig$Hour) , ]

#Limit Peak Days in June, July, August
load.summer <- subset(load_hist.orig, months(dateTime) %in% c("June", "July", "August") &
                                      !isWkEnd )
#save the last 186 days for validation.  First 500 for training/testing:
# 2010-07-19

#drop 2013 for testing
load.validate <- subset(load.summer, as.Date(Date) > "2010-07-09"  )
load.summer <- subset( load.summer, as.Date(Date) <= "2010-07-09")
rm(load_hist.orig)

#Reorganize the data set into hourly data with appropriate averages.
fun.cast.hourly = function(df)
{
  load.hour <- cast(load.summer, Date ~ Hour, value="DEMAND")
  drytemp.hour <- cast(load.summer, Date ~ Hour, value="DryBulb")
  wettemp.hour <- cast(load.summer, Date ~ Hour, value="DewPnt")
  load.hour <- data.frame(load.hour, PK=rowMeans(load.hour[, 9:24]), 
                                  OFF=rowMeans(load.hour[, c(2:8, 25)]), 
                                  BASE=rowMeans(load.hour), 
                                  SPK = rowMeans(load.hour[, 13:16]), 
                                  DRY_PK = rowMeans(drytemp.hour[ , 9:24]), 
                                  DRY_OFF = rowMeans(drytemp.hour[, c(2:8, 25)]), 
                                  DRY_BASE = rowMeans(drytemp.hour),
                                  DRY_SPK = rowMeans(drytemp.hour[, 13:16]), 
                                  WET_PK = rowMeans(wettemp.hour[ , 9:24]), 
                                  WET_OFF = rowMeans(wettemp.hour[, c(2:8, 25)]), 
                                  WET_BASE = rowMeans(wettemp.hour),
                                  WET_SPK = rowMeans(wettemp.hour[, 13:16]) )   
  return(load.hour)
}

load.hour <- fun.cast.hourly(load.summer) 
load.val <- fun.cast.hourly(load.validate)

```

The rough hourly shapes by hour and temperature.
```{r, echo=FALSE, message=FALSE, fig.width=5, fig.height=5}
ggplot(aes(x=Hour, y=DEMAND), data=load.summer) + 
  geom_line(aes(group=Date), alpha=.1) + 
  geom_point(aes(color=DryBulb), alpha=.1) + 
  theme_bw() + 
  geom_smooth(color="red") + scale_color_gradientn(colours=rainbow(7)) +
  labs(title="Hourly Load Profiles (Summer, Weekday)")

#simpler version of daily load profiles
ggplot(aes(x=Hour, y=DEMAND), data=load.summer) + 
  geom_line(aes(group=Date), alpha=.1) + 
  geom_point(alpha=.1) + 
  theme_bw() + 
  geom_smooth(color="red") +
  labs(title="Hourly Load Profiles (Summer, Weekday)")


ggplot(aes(x=DryBulb, y=DEMAND), data=load.summer) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth() + 
  labs(title="Load by Hourly Temperature")

###A simpler plot with temperature dependence.
ggplot(aes(x=DRY_SPK, y=SPK), data=load.hour) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth() +
  labs(title="Load by Hourly Temperature")

ggplot(aes(x=dateTime, y=DEMAND), data=load.summer[10200:10600, ]) + 
  geom_point() +
  theme_bw() + 
  geom_line()
       
#Create a data-set to be used in the python script
#First row is the average performance over the historical data
#Remaining rows are the validate test set
load.python <- cast(load.validate, Date~Hour, value="DEMAND")
mean.load = colMeans(load.hour[, 2:37], na.rm=TRUE)
t = rbind(load.python, c("2010-06-01", mean.load))
load.python = t[c(193, 1:192), 2:25]

write.csv(load.python, file="load_validate_set.csv")

load.python.temp = cast(load.validate, Date~Hour, value="DryBulb")
t = cast(load.summer, Date~Hour, value="DryBulb")
mean.temp = colMeans(t[, 2:25], na.rm=TRUE)
load.python.temp = rbind(load.python.temp, c("2010-06-01", mean.temp))
write.csv(load.python.temp, file="load_validate_set_hourly_temps.csv")

#Clean up some useless objects
rm(load_validate, load.summer)
```

The following plot will be the basis of our comparisons.  Compares Off-Pk average vs. Super-Peak average.
```{r echo=FALSE}
#Build teh fundamental base plot
g  = ggplot(aes(x=OFF, y=SPK), data=load.hour) + 
  geom_point(aes(color=DRY_BASE, size=DRY_BASE)) + 
  theme_bw() + theme(legend.title=element_blank() ) + scale_color_gradientn(colours=rainbow(7))

ggplot(aes(x=OFF, y=SPK), data=load.hour) + 
  geom_point(aes(size=DRY_BASE + 2), color="black") + 
  geom_point(aes(color=DRY_BASE, size=DRY_BASE)) + 
  theme_bw() + theme(legend.position="none" )

```

The CLT Set
------------------
The hourly loads aren't symmetrically distributed.  We consider approximating by 
3 standard deviations around the mid-point.  
```{r echo=FALSE }
#Compute the relevant statistics to form the CLT set
#Mean by Hour
dbar = apply(load.hour[, 2:25], 2, mean, na.rm=TRUE)
dlow = apply(load.hour[, 2:25], 2, min, na.rm=TRUE)
dhigh = apply(load.hour[, 2:25], 2, max, na.rm=TRUE)

#compute the maximal distances for dhat
dhat = apply(cbind(dhigh - dbar, dbar - dlow), 1, max)

#alternatively, compute std
d_std = apply(load.hour[, 2:25], 2, sd, na.rm=TRUE)

t= data.frame(Avg=dbar, Low=dlow, High=dhigh, Mid=dhat, Std=d_std)
t = data.frame(Hour=1:24, t)
t2 = melt(t, id.vars="Hour")

#Take a look at the shapes to get a sense of what's going on.  
ggplot(aes(x=Hour, y=value, color=variable, group=variable), data=t2) + 
  geom_line() + geom_point() + theme_bw()

````

We can project the CLT set onto the above Off-Pk vs. SPK graph.
```{r echo=FALSE}
#These were computed in matlab
#The individual bounds are 3 times the standard deviation.
#Note that perturbations s large as 3.5-4 times the std dev observed in data set
#Gamma = 3 sqrt(N)
spk_off_clt = read.csv("/Users/VGupta/Documents/Research/UnitCommittment/ISO-NE/spk_off")
names(spk_off_clt) <- c("OFF", "SP_MIN", "SP_MAX")

g + geom_line(aes(x=OFF, y=SP_MIN), data=subset(spk_off_clt, SP_MIN < 1e8)) + 
  geom_line(aes(x=OFF, y = SP_MAX), data=subset(spk_off_clt, SP_MAX > -1e-8)) + 
  geom_segment(x = min(spk_off_clt$OFF), y = spk_off_clt$SP_MIN[1], 
                   xend = min(spk_off_clt$OFF), yend = spk_off_clt$SP_MAX[1]) + 
  geom_segment(x = max(spk_off_clt$OFF), y = spk_off_clt$SP_MIN[20], 
                   xend = max(spk_off_clt$OFF), yend = spk_off_clt$SP_MAX[20]) 
```


The UCS Set
------------
The UCS is a refinement of the Ellipsoid Heuristic.  We compute in matlab and 
overlay a rough set here.  
```{r echo=FALSE}
#Write out the data to do some analysis in matlab
#write.csv(subset(load.hour, !is.na(X1) & !is.na(X24) ), "summerPkHourlyLoad.csv")

# The computations for the UCS Set were done in Matlab
spk_off_ucs = read.csv("/Users/VGupta/Documents/Research/UnitCommittment/ISO-NE/spk_off_ucs")
names(spk_off_ucs) <- c("OFF", "SP_MIN", "SP_MAX")

spk_off_ucs = subset(spk_off_ucs, SP_MIN < 1e9, SP_MAX > 0)

g + geom_point(aes(x=OFF, y=SP_MIN), data=spk_off_ucs) + 
  geom_point(aes(x=OFF, y=SP_MAX), data=spk_off_ucs)
```

Regression Based Approach
--------------------------
We now consider first regressing on temperature indicators and then
fitting a set for the residuals.  (Pointwise estimates).  Ideally, we'd regress on **forecasted** temperature, but don't have access to the data yet.  
```{r echo=FALSE}
#This section just sets up some functions that will be useful later.
mse= function(sim, obs)
{
  filt = is.na(sim) | is.na(obs)
  mean((sim[!filt] - obs[!filt])^2)
}

assessModel=function( model, tag, df_out, iHr, val_df)
{
  adjr2 = summary(model)$adj.r.squared
  out_mse = mse(val_df[, 1 + iHr], predict(model, val_df) )
  in_mse = mse(model$fitted.values, model$fitted.values + model$residuals )
  out_adjr2 = 1 - var(val_df[, 1 + iHr] -  predict(model, val_df), na.rm=TRUE) / 
    var( val_df[, 1 + iHr], na.rm=TRUE)     
  
  df_out = rbind(df_out, data.frame(Hour=c(iHr), Method=c(tag), IN_AdjR2=c(adjr2), OUT_MSE=c(out_mse), IN_MSE=c(in_mse), OUT_AdjR2=c(out_adjr2)))
  return(df_out)
}
```

```{r echo=FALSE}
#These are all failed, simple regressions that we got rid of.
#do the simplest regression (on Base Temp)
# t_fun = function(load_col){ lm(load_col ~ DRY_BASE + I(DRY_BASE^2), data=load.hour)}
# temp.regs.base = apply(load.hour[, 2:29], MARGIN=2, FUN=t_fun)
# 
# t_fun = function(load_col){ lm(load_col ~ DRY_PK + I(DRY_PK^2), data=load.hour)}
# temp.regs.pk = apply(load.hour[, 2:29], MARGIN=2, FUN=t_fun)
# 
# t_fun = function(load_col){ lm(load_col ~ DRY_OFF + I(DRY_OFF^2), data=load.hour)}
# temp.regs.off = apply(load.hour[, 2:29], MARGIN=2, FUN=t_fun)

# t_fun = function(load_col){ lm(load_col ~ DRY_SPK + I(DRY_SPK^2), data=load.hour)}
# temp.regs.spk = apply(load.hour[, 2:29], MARGIN=2, FUN=t_fun)

# t_fun = function(load_col){ lm(load_col~ (DRY_SPK + DRY_OFF)^2 + I(DRY_SPK)^2 + I(DRY_OFF)^2, 
#                                data=load.hour)}
# temp.regs.spkOff = apply(load.hour[, 2:29], MARGIN=2, FUN=t_fun)

# t_fun = function(load_col){ lm(load_col~ (DRY_SPK + DRY_BASE)^2 + I(DRY_SPK)^2 + I(DRY_BASE)^2, 
#                                data=load.hour)}
# temp.regs.spkBase = apply(load.hour[, 2:29], MARGIN=2, FUN=t_fun)


#t_fun = function(load_col){ lm(load_col~ (DRY_PK + DRY_OFF)^2 + I(DRY_PK)^2 + I(DRY_OFF)^2, 
#                               data=load.hour)}
#temp.regs.pkOff = apply(load.hour[, 2:29], MARGIN=2, FUN=t_fun)


```

```{r echo=FALSE, message=FALSE, results='hide'}
#Perfors a lagged regression.  Not a fair comparison.  
#Try a regression with two lags on the pk and off pk data sets
#Requires dropping the first couple values.
t_dat = load.hour
temp_dat = data.frame(embed(load.hour$DRY_PK, 3), embed(load.hour$DRY_OFF, 3), embed(load.hour$DRY_SPK, 3)) 
names(temp_dat) <- c(paste("DRY_PK", 0:2, sep=""), paste("DRY_OFF", 0:2, sep=""), paste("DRY_SPK", 0:2, sep=""))
t_dat = data.frame(load.hour[3:531, ], temp_dat)

t_fun = function(load_col)
{
  t_dat = na.omit(data.frame(load=load_col, temp_dat))
  reg.lag = lm(load ~ .^2 + I(DRY_PK0^2) + I(DRY_OFF0^2) + I(DRY_SPK0^2), 
     data=t_dat)
  return (step(reg.lag))
}
temp.regs.lagged = apply(load.hour[3:531, 2:29], MARGIN=2, FUN=t_fun)
```

These are more informative regressions we kept.
```{r, echo=FALSE, message=FALSE, results='hide' }
t_fun = function(load_col){ lm(load_col ~ DRY_OFF + DRY_PK + 
                 I(DRY_PK^2) + I(DRY_OFF^2), data=load.hour)}
temp.regs.trial3 = apply(load.hour[, 2:29], MARGIN=2, FUN=t_fun)

#The gold standard will be the step function on everything
t_fun = function(load_col)
{ 
  step( lm(load_col ~ (DRY_BASE + DRY_PK + DRY_SPK + WET_PK + WET_BASE + WET_SPK)^2 + 
                I(DRY_BASE^2) + I(DRY_PK^2) + I(DRY_SPK^2) + 
                I(WET_BASE^2) + I(WET_PK^2) + I(WET_SPK^2), 
              data=load.hour)) 
}
temp.regs.step = apply(load.hour[, 2:29], MARGIN=2, FUN=t_fun)
```

```{r echo=FALSE}
#Do some assessments on the models
#First need to augment the test set
temp_dat = data.frame(embed(load.test$DRY_PK, 3), embed(load.test$DRY_OFF, 3), embed(load.test$DRY_SPK, 3)) 
names(temp_dat) <- c(paste("DRY_PK", 0:2, sep=""), paste("DRY_OFF", 0:2, sep=""), paste("DRY_SPK", 0:2, sep=""))
load.test = data.frame(load.test[3:531, ], temp_dat)


data_out = NULL
for (iHr in 1:24)
{
  data_out = assessModel(temp.regs.step[[iHr]], "STEP", data_out, iHr, load.test)
  data_out = assessModel(temp.regs.trial3[[iHr]], "TRIAL3", data_out, iHr, load.test)
  data_out = assessModel(temp.regs.lagged[[iHr]], "Lagged", data_out, iHr, load.test)
}

#plot the R2 values
ggplot(aes(x=Hour, color=Method, y=IN_AdjR2, label=Hour), data=data_out) + 
  geom_line() + geom_point() + theme_bw() +
  geom_line(aes(y=OUT_AdjR2), linetype="dashed") + geom_text()

#plot the MSE in and out
ggplot(aes(x=Hour, y=OUT_MSE, label=Hour, color=Method), data=data_out) + geom_line() + 
  geom_point() + theme_bw() + 
  geom_line(aes(y=IN_MSE), linetype="dashed") + geom_text()

#Deep dive into some of the hours to improve model
model = temp.regs.lagged[[8]]
```

**Observations**
- Something Strange is happening at hour 8 and 21 for the $R^2$ values
- Similraly somethinig strange between 5-8 and 15-21 in the Mean Square Error.


We use the (independent) test set to compute the residuals and construct a 
UCS set for these residuals in Matlab. 
```{r echo=FALSE}
calcResids = function(models)
{
  t <- NULL  
  for (iHr in 1:24)
  {
    t<- cbind(t, load.test[, 1 + iHr] - predict(models[[iHr]], load.test) )
  }
  return(t)
}

out.resid.lagged <- data.frame(calcResids(temp.regs.lagged))
out.resid.step <- data.frame(calcResids(temp.regs.step))
out.resid.trial <- data.frame(calcResids(temp.regs.trial3))

ggplot(aes(x=X21), data=out.resid.lagged) + geom_histogram() + theme_bw()


#write out the data with the fit to matlab.  
write.csv(subset(out.resid.step, !is.na(X1) & ! is.na(X24)), "residualsTrial.csv")

```

Plot the corresponding Linear regression set....
``` {r }
#read in the epsilon ellipsoid from matlab
eps.ellipse = read.csv('/Users/VGupta/Documents/Research/UnitCommittment/ISO-NE/UEps_LinReg.csv')
fit.loads = data.frame(lapply(temp.regs.step, function(model){ predict(model, load.hour)}))

fit.loads <- fit.loads[, 1:24]
fit.loads <- data.frame(fit.loads, PK=rowMeans(fit.loads[, 8:23]), 
                                  OFF=rowMeans(fit.loads[, c(1:7, 24)]), 
                                  BASE=rowMeans(fit.loads), 
                                  SPK = rowMeans(fit.loads[, 12:15])) 

eps_off = load.hour[, "OFF"] - fit.loads[, "OFF"]  
eps_spk_min = laply(eps_off,
      function(eps){ eps.ellipse[max(which(eps.ellipse[, 1] <= eps)), 2]   })
eps_spk_max = laply(eps_off,
      function(eps){ eps.ellipse[max(which(eps.ellipse[, 1] <= eps)), 3]   })
fit.loads <- data.frame(fit.loads, EPS_MAX = eps_spk_max, EPS_MIN=eps_spk_min)


g.linreg <- ggplot(aes(x=OFF, y=SPK), data=load.hour) + 
  geom_point(aes(color=DRY_SPK)) + theme_bw() + 
  geom_pointrange(aes(x=OFF, y=SPK,  
                      ymin=SPK + eps_spk_min, ymax=SPK + eps_spk_max), 
                  alpha=.1)

g.linreg + geom_point(aes(x=OFF, y=SP_MIN), data=spk_off_ucs) + 
    geom_point(aes(x=OFF, y=SP_MAX), data=spk_off_ucs)

```

```{r }
#Fit some similar models using CART
#exploration first
# cart.pkoff = rpart(.0001 * OFF ~ DRY_PK + DRY_OFF, data=load.hour, method='anova')
# cart.pkoff.prune = prune(cart.pkoff, cp=.01)
# prp(x=cart.pkoff.prune, type=2, extra=100, under=TRUE, digits=3)
```